# ğŸ‡ªğŸ‡¸â†’ğŸ‡©ğŸ‡° Traductor EspaÃ±ol â†’ DanÃ©s (NLLB + CTranslate2)

**Servicio de traducciÃ³n 100% local, gratuito y privado** utilizando el modelo NLLB (No Language Left Behind) de Meta con cuantizaciÃ³n INT8 via CTranslate2.

---

## ğŸ¯ CaracterÃ­sticas

âœ… **100% Offline** - Sin llamadas a Internet (excepto descarga inicial del modelo)  
âœ… **Gratuito** - Modelo open source, sin lÃ­mites de uso  
âœ… **Privado** - Tus datos nunca salen de tu mÃ¡quina  
âœ… **Optimizado** - CuantizaciÃ³n INT8 para inferencia eficiente en CPU  
âœ… **Glosarios personalizados** - Control terminolÃ³gico para dominios especÃ­ficos  
âœ… **Batch processing** - Traduce mÃºltiples textos simultÃ¡neamente  
âœ… **API REST** - FÃ¡cil integraciÃ³n con FastAPI  

---

## ğŸ“‹ Requisitos

### Hardware
- **RAM mÃ­nima**: 
  - 8 GB para modelo `nllb-200-distilled-600M` (recomendado)
  - 16 GB para modelo `nllb-200-1.3B` (mejor calidad)
- **Espacio en disco**: ~3-6 GB (modelo + conversiÃ³n)
- **CPU**: Cualquier procesador moderno (optimizado para CPU)

### Software
- Python 3.11+
- pip
- (Opcional) Docker para despliegue containerizado

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### OpciÃ³n 1: InstalaciÃ³n Local (Recomendada)

#### 1. Clonar repositorio e instalar dependencias

```bash
# Crear entorno virtual e instalar dependencias
make venv

# O manualmente:
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

pip install -r requirements.txt
```

#### 2. Descargar modelo

```bash
# Descargar modelo 600M (recomendado para 8GB+ RAM)
make download

# O modelo 1.3B (mejor calidad, requiere 16GB+ RAM)
make download-1.3b

# O manualmente:
python scripts/download_model.py \
    --model facebook/nllb-200-distilled-600M \
    --out models/nllb-600m
```

**Modelos disponibles:**
- `facebook/nllb-200-distilled-600M` - ~2.4 GB (recomendado)
- `facebook/nllb-200-1.3B` - ~5 GB (mejor calidad)
- `facebook/nllb-200-3.3B` - ~13 GB (requiere mucha RAM)

#### 3. Convertir a CTranslate2 INT8

```bash
make convert

# O manualmente:
bash scripts/convert_to_ct2.sh \
    --in models/nllb-600m \
    --out models/nllb-600m-ct2-int8
```

#### 4. Ejecutar servidor

```bash
make run

# O manualmente:
uvicorn app.app:app --host 0.0.0.0 --port 8000
```

El servidor estarÃ¡ disponible en:
- **API**: http://localhost:8000
- **DocumentaciÃ³n interactiva**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

---

### OpciÃ³n 2: Docker

#### 1. Preparar modelos (en host)

```bash
# Descargar y convertir modelo (solo una vez)
make download
make convert
```

#### 2. Construir imagen Docker

```bash
make docker-build

# O manualmente:
docker build -t traductor-es-da:latest .
```

#### 3. Ejecutar contenedor

```bash
make docker-run

# O manualmente:
docker run -d \
  --name traductor-es-da \
  -p 8000:8000 \
  -v $(pwd)/models:/models:ro \
  -e MODEL_DIR=/models/nllb-600m \
  -e CT2_DIR=/models/nllb-600m-ct2-int8 \
  traductor-es-da:latest
```

**Notas sobre Docker:**
- Los modelos **NO** estÃ¡n incluidos en la imagen (son muy grandes)
- Debes montarlos como volumen: `-v ./models:/models`
- El flag `:ro` (read-only) previene modificaciones accidentales

---

## ğŸ“– Uso del API

### Ejemplo bÃ¡sico

```bash
curl -X POST http://localhost:8000/translate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hola mundo",
    "max_new_tokens": 256
  }'
```

**Respuesta:**
```json
{
  "provider": "nllb-ct2-int8",
  "source": "spa_Latn",
  "target": "dan_Latn",
  "translations": ["Hej verden"]
}
```

### TraducciÃ³n de mÃºltiples textos (batch)

```bash
curl -X POST http://localhost:8000/translate \
  -H "Content-Type: application/json" \
  -d '{
    "text": [
      "Buenos dÃ­as",
      "Â¿CÃ³mo estÃ¡s?",
      "Gracias por tu ayuda"
    ],
    "max_new_tokens": 256
  }'
```

**Respuesta:**
```json
{
  "provider": "nllb-ct2-int8",
  "source": "spa_Latn",
  "target": "dan_Latn",
  "translations": [
    "God morgen",
    "Hvordan har du det?",
    "Tak for din hjÃ¦lp"
  ]
}
```

### Uso de glosario personalizado

```bash
curl -X POST http://localhost:8000/translate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Bienvenido a Acme Corporation",
    "glossary": {
      "Acme": "Acme",
      "Corporation": "Selskab"
    }
  }'
```

El glosario permite:
- **Preservar tÃ©rminos** (nombres propios, marcas, etc.)
- **Forzar traducciones especÃ­ficas** (terminologÃ­a de dominio)
- **Control de consistencia** en documentos tÃ©cnicos

**Nota:** Los tÃ©rminos del glosario se reemplazan de forma case-insensitive y con word boundaries para evitar matches parciales.

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno

Copia `env.example` a `.env` y ajusta segÃºn necesites:

```bash
# Modelo a usar
MODEL_NAME=facebook/nllb-200-distilled-600M

# Directorios
MODEL_DIR=./models/nllb-600m
CT2_DIR=./models/nllb-600m-ct2-int8

# ConfiguraciÃ³n de hilos CTranslate2
# 0 = automÃ¡tico (usa todos los cores)
CT2_INTER_THREADS=0
CT2_INTRA_THREADS=0

# Batch size por defecto
DEFAULT_BATCH_SIZE=16
```

### Ajuste de Performance

#### Hilos de CPU

CTranslate2 usa dos tipos de paralelismo:

- **`inter_threads`**: Paralelismo entre traducciones (batch processing)
- **`intra_threads`**: Paralelismo dentro de cada traducciÃ³n

ConfiguraciÃ³n recomendada:
```bash
# Sistema con 8+ cores: dejar en automÃ¡tico
CT2_INTER_THREADS=0
CT2_INTRA_THREADS=0

# Sistema con 4 cores: limitar para evitar overhead
CT2_INTER_THREADS=2
CT2_INTRA_THREADS=2
```

#### Batch Size

Para mejorar throughput con mÃºltiples textos:

```python
# En tu cÃ³digo
response = requests.post(
    "http://localhost:8000/translate",
    json={
        "text": ["texto1", "texto2", ..., "texto16"],  # Batch de 8-16
        "max_new_tokens": 256
    }
)
```

---

## ğŸ§ª Testing

```bash
# Ejecutar todos los tests
make test

# Tests con output detallado
make test-verbose

# O con pytest directamente
pytest tests/ -v -s
```

**Smoke tests incluidos:**
- âœ… TraducciÃ³n de texto simple
- âœ… TraducciÃ³n batch
- âœ… Uso de glosario
- âœ… ValidaciÃ³n de parÃ¡metros
- âœ… Health checks

---

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py           # FastAPI app con endpoint /translate
â”‚   â”œâ”€â”€ schemas.py       # Modelos Pydantic (request/response)
â”‚   â”œâ”€â”€ inference.py     # Motor de inferencia CT2 + tokenizador
â”‚   â””â”€â”€ glossary.py      # Funciones de glosario pre/post
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_model.py     # Descarga de modelos HF
â”‚   â””â”€â”€ convert_to_ct2.sh     # ConversiÃ³n a CTranslate2 INT8
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_translate_smoke.py
â”œâ”€â”€ models/              # Modelos (ignorado en git)
â”‚   â”œâ”€â”€ nllb-600m/              # Modelo HuggingFace
â”‚   â””â”€â”€ nllb-600m-ct2-int8/     # Modelo CTranslate2
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ env.example
â””â”€â”€ README.md
```

---

## ğŸ“ Detalles TÃ©cnicos

### Â¿Por quÃ© CTranslate2?

**CTranslate2** es un motor de inferencia optimizado para modelos Transformer:

- âœ… **2-4x mÃ¡s rÃ¡pido** que HuggingFace Transformers
- âœ… **Menor uso de RAM** con cuantizaciÃ³n INT8
- âœ… **Optimizaciones para CPU** (SIMD, multithreading)
- âœ… **Compatible** con modelos NLLB sin cambios

### CuantizaciÃ³n INT8

La cuantizaciÃ³n reduce el tamaÃ±o del modelo y acelera la inferencia:

- **FP32 (original)**: 2.4 GB â†’ ~600M parÃ¡metros Ã— 4 bytes
- **INT8 (cuantizado)**: ~600 MB â†’ ~600M parÃ¡metros Ã— 1 byte
- **PÃ©rdida de calidad**: < 1% (imperceptible en la mayorÃ­a de casos)

### CÃ³digos de Idioma FLORES-200

NLLB usa cÃ³digos FLORES-200 para 200+ idiomas:

- **EspaÃ±ol**: `spa_Latn` (script latino)
- **DanÃ©s**: `dan_Latn` (script latino)

### Estrategia de Glosario

ImplementaciÃ³n conservadora para preservar tÃ©rminos:

1. **Pre-procesamiento**: Marca tÃ©rminos ES con `[[TERM::<texto>]]`
2. **TraducciÃ³n**: El modelo ve los marcadores y tiende a preservarlos
3. **Post-procesamiento**: Reemplaza marcadores por tÃ©rminos DA

**Limitaciones:**
- Puede alterar puntuaciÃ³n adyacente
- TÃ©rminos multi-palabra pueden causar problemas
- No garantiza 100% preservaciÃ³n (depende del modelo)

---

## ğŸ› Troubleshooting

### Error: "Modelo no encontrado"

```bash
# Verifica que los modelos existan
make info

# Si no existen, descarga y convierte
make download
make convert
```

### Error: "Out of memory" (RAM insuficiente)

```bash
# Prueba con el modelo mÃ¡s pequeÃ±o
make download MODEL_NAME=facebook/nllb-200-distilled-600M

# O reduce max_new_tokens en la request
{
  "text": "...",
  "max_new_tokens": 128  # En lugar de 256
}
```

### Traducciones lentas

```bash
# Ajusta hilos de CPU
export CT2_INTER_THREADS=4
export CT2_INTRA_THREADS=4

# Usa batch processing
# (agrupa mÃºltiples textos en una sola request)
```

### Docker: "models not found"

```bash
# AsegÃºrate de montar el volumen correctamente
docker run -v $(pwd)/models:/models ...

# Verifica que los modelos existan en ./models/
ls -lh models/
```

---

## ğŸ“Š Benchmarks (Aproximados)

Hardware: Intel i7-10700K (8 cores), 32 GB RAM

| Modelo          | RAM Uso | Velocidad | Calidad (BLEU) |
|-----------------|---------|-----------|----------------|
| 600M (INT8)     | ~2 GB   | ~20 tok/s | Buena          |
| 1.3B (INT8)     | ~4 GB   | ~12 tok/s | Muy buena      |
| 3.3B (INT8)     | ~10 GB  | ~5 tok/s  | Excelente      |

**Nota:** Los benchmarks varÃ­an segÃºn hardware, longitud de texto y configuraciÃ³n de hilos.

---

## ğŸ›£ï¸ Roadmap

- [ ] Soporte para mÃ¡s pares de idiomas (configurable)
- [ ] API streaming para textos largos
- [ ] Cache de traducciones frecuentes
- [ ] UI web simple para pruebas
- [ ] Soporte para modelos cuantizados a INT4
- [ ] MÃ©tricas de calidad (BLEU, COMET)

---

## ğŸ“œ Licencia

Este proyecto es de cÃ³digo abierto. El modelo NLLB estÃ¡ licenciado bajo la licencia de Meta (Creative Commons).

**Modelos utilizados:**
- NLLB-200: https://github.com/facebookresearch/fairseq/tree/nllb
- Licencia: CC-BY-NC 4.0 (uso no comercial)

---

## ğŸ™ Agradecimientos

- **Meta AI** - Por el modelo NLLB (No Language Left Behind)
- **OpenNMT** - Por CTranslate2
- **HuggingFace** - Por Transformers y el Hub
- **FastAPI** - Por el framework web

---

## ğŸ“ Soporte

Para problemas o preguntas:

1. Revisa la secciÃ³n de [Troubleshooting](#-troubleshooting)
2. Verifica los logs: `docker logs traductor-es-da` (Docker) o consola (local)
3. Ejecuta `make info` para ver el estado del sistema

---

## ğŸ¯ Ejemplos de Uso

### Python

```python
import requests

# TraducciÃ³n simple
response = requests.post(
    "http://localhost:8000/translate",
    json={"text": "Hola mundo"}
)
print(response.json()["translations"][0])  # "Hej verden"

# Batch con glosario
response = requests.post(
    "http://localhost:8000/translate",
    json={
        "text": [
            "Bienvenido a Python",
            "FastAPI es genial"
        ],
        "glossary": {
            "Python": "Python",
            "FastAPI": "FastAPI"
        }
    }
)
for translation in response.json()["translations"]:
    print(translation)
```

### JavaScript/Node.js

```javascript
const axios = require('axios');

async function translate(text) {
  const response = await axios.post('http://localhost:8000/translate', {
    text: text,
    max_new_tokens: 256
  });
  return response.data.translations[0];
}

translate("Hola mundo").then(console.log);  // "Hej verden"
```

### cURL

```bash
# Test rÃ¡pido
make curl-test

# O manualmente
curl -X POST http://localhost:8000/translate \
  -H "Content-Type: application/json" \
  -d '{"text": "Hola mundo"}' | jq
```

---

**Â¡Disfruta de la traducciÃ³n local, gratuita y privada! ğŸš€**


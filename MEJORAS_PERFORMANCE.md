# Mejoras de Performance y Anti-Truncado

## ğŸ“‹ Resumen

Este documento describe las mejoras implementadas para:
1. **Evitar traducciones cortadas** de frases largas
2. **Hacer la aplicaciÃ³n mÃ¡s fluida** y responsiva

## ğŸ¯ Cambios Implementados

### 1. Backend - Anti-Truncado

#### 1.1 LÃ­mites de entrada aumentados

**Archivo**: `app/settings.py`

```python
# Antes
MAX_INPUT_TOKENS: int = 384
MAX_NEW_TOKENS: int = 192

# DespuÃ©s
MAX_INPUT_TOKENS: int = 1024   # +166% para textos largos
MAX_NEW_TOKENS: int = 256       # +33% para salidas completas
```

**Impacto**:
- âœ… Textos de hasta ~3000 caracteres sin truncado
- âœ… Traducciones completas sin cortes abruptos
- âœ… Mejor manejo de emails y documentos largos

#### 1.2 max_new_tokens Adaptativo

**Archivo**: `app/inference.py`

Nueva funciÃ³n `_derive_max_new_tokens()` que calcula automÃ¡ticamente el nÃºmero de tokens de salida basado en la longitud de entrada:

```python
def _derive_max_new_tokens(input_lengths: List[int]) -> int:
    """
    HeurÃ­stica: salida ~ 1.2x del input mÃ¡s largo
    LÃ­mite: 128-512 (segÃºn schema Pydantic)
    """
    max_input_len = max(input_lengths)
    estimated = int(max_input_len * 1.2)
    return max(128, min(512, estimated))
```

**Comportamiento**:
- Si el cliente **NO especifica** `max_new_tokens`, el backend lo calcula automÃ¡ticamente
- Si el cliente **especifica** un valor, se respeta
- Rango vÃ¡lido: 32-512 tokens (validado por Pydantic)

**Impacto**:
- âœ… Traducciones mÃ¡s completas sin intervenciÃ³n manual
- âœ… OptimizaciÃ³n automÃ¡tica segÃºn longitud de entrada
- âœ… Menos errores de truncado

#### 1.3 SegmentaciÃ³n Inteligente

**Archivo**: `app/segment_text.py` (nuevo)

MÃ³dulo reutilizable para segmentaciÃ³n de texto plano:

```python
def split_text_for_plain(text: str, max_segment_chars: int = 800) -> List[str]:
    """
    Segmenta texto largo preservando oraciones y pÃ¡rrafos.
    Reutiliza split_text_for_email (ya testeado).
    """
```

**ActualizaciÃ³n en `app/app.py`**:
- SegmentaciÃ³n con `max_segment_chars=800` (antes: 600)
- Usa `settings.MAX_SEGMENT_CHARS` para configurabilidad

**Impacto**:
- âœ… Menos segmentos â†’ menos overhead
- âœ… Traducciones mÃ¡s coherentes
- âœ… Preserva contexto entre oraciones

### 2. Frontend - Control UI y Fluidez

#### 2.1 Control de Max Tokens

**Archivos**: 
- `frontend/src/store/useAppStore.ts`
- `frontend/src/components/TextTranslator.tsx`
- `frontend/src/components/HtmlTranslator.tsx`

**Nuevo control UI**:

```tsx
<div className="flex items-center gap-2">
  <Label htmlFor="max-tokens">Max tokens:</Label>
  <input
    id="max-tokens"
    type="number"
    min={32}
    max={512}
    step={16}
    value={maxNewTokens}
    onChange={...}
  />
</div>
```

**CaracterÃ­sticas**:
- ğŸšï¸ Control numÃ©rico visible en ambas pestaÃ±as (Texto y HTML)
- ğŸ’¾ Persistido en `localStorage`
- âœ… ValidaciÃ³n cliente: 32-512
- ğŸ”¢ Default: 256 (actualizado de 192)

**Impacto**:
- âœ… Usuario puede ajustar segÃºn necesidad
- âœ… Feedback visual del valor actual
- âœ… ConfiguraciÃ³n persistente entre sesiones

#### 2.2 MediciÃ³n de Latencia Corregida

**Archivo**: `frontend/src/hooks/useTranslate.ts`

```typescript
const startTime = performance.now()
// ... traducir ...
const totalTime = Math.round(performance.now() - startTime)
setLastSuccess(`âœ“ TraducciÃ³n completada en ${totalTime}ms`)
```

**Impacto**:
- âœ… MÃ©trica precisa de tiempo total
- âœ… Feedback inmediato al usuario
- âœ… Visible en barra de estado inferior

### 3. Esquemas y ValidaciÃ³n

**Archivo**: `app/schemas.py`

```python
max_new_tokens: int = Field(
    default=256,  # actualizado de 192
    ge=32,
    le=512,
    description="NÃºmero mÃ¡ximo de tokens a generar"
)
```

**Impacto**:
- âœ… Default mÃ¡s alto para textos largos
- âœ… ValidaciÃ³n consistente backend/frontend
- âœ… DocumentaciÃ³n API actualizada

## ğŸ“Š Comparativa Antes/DespuÃ©s

| MÃ©trica | Antes | DespuÃ©s | Mejora |
|---------|-------|---------|--------|
| MAX_INPUT_TOKENS | 384 | 1024 | +166% |
| MAX_NEW_TOKENS (default) | 192 | 256 | +33% |
| max_segment_chars | 600 | 800 | +33% |
| Texto mÃ¡ximo sin truncar | ~1500 chars | ~3000 chars | +100% |
| max_new_tokens | Fijo | Adaptativo | â™¾ï¸ |
| Control UI | âŒ | âœ… | âœ“ |

## ğŸ§ª Tests

**Archivo**: `tests/test_long_text_plain.py` (nuevo)

Suite de tests para verificar las mejoras:

1. âœ… `test_plain_long_text_not_truncated`
   - Texto de ~3400 caracteres
   - Verifica traducciÃ³n completa sin truncado

2. âœ… `test_multiple_long_texts_batch`
   - MÃºltiples textos largos en un request
   - Verifica procesamiento batch correcto

3. âœ… `test_adaptive_max_new_tokens`
   - Verifica cÃ¡lculo adaptativo
   - Compara texto corto vs largo

4. âœ… `test_segmentation_preserves_meaning`
   - Texto con mÃºltiples pÃ¡rrafos
   - Verifica que segmentaciÃ³n respeta estructura

5. âœ… `test_max_new_tokens_boundary`
   - Verifica lÃ­mites 32-512
   - Valida error 422 para valores fuera de rango

**Ejecutar tests**:

```bash
# Todos los tests
pytest tests/test_long_text_plain.py -v

# Test especÃ­fico
pytest tests/test_long_text_plain.py::test_plain_long_text_not_truncated -v
```

## ğŸš€ Uso

### Backend

**Sin cambios necesarios** - Mejoras automÃ¡ticas:

```json
{
  "text": "Texto muy largo...",
  "direction": "es-da"
}
```

Backend automÃ¡ticamente:
- Calcula `max_new_tokens` Ã³ptimo
- Segmenta si es necesario
- Traduce sin truncar

**Con control manual**:

```json
{
  "text": "Texto muy largo...",
  "direction": "es-da",
  "max_new_tokens": 384
}
```

### Frontend

1. **Ajuste de Max Tokens**:
   - Visible en header de ambas pestaÃ±as
   - Ajustar con â†‘â†“ o escribir valor
   - Rango: 32-512

2. **Textos largos**:
   - Pegar texto (hasta ~50k caracteres)
   - Backend segmenta automÃ¡ticamente
   - TraducciÃ³n completa sin cortes

3. **MÃ©tricas**:
   - Latencia visible en barra inferior
   - Cache hits/misses
   - Contador de caracteres con advertencia >10k

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

```bash
# .env o config.env
MAX_INPUT_TOKENS=1024        # LÃ­mite de entrada
MAX_NEW_TOKENS=256           # Default de salida
MAX_SEGMENT_CHARS=800        # TamaÃ±o de segmentos
BEAM_SIZE=3                  # Conservador para estabilidad
```

### Ajuste de Performance

Para **textos muy largos** (>5000 caracteres):

1. Aumentar `MAX_SEGMENT_CHARS` a 1000-1200
2. Aumentar `max_new_tokens` a 384-512
3. Considerar procesamiento en mÃºltiples requests

Para **latencia baja** (textos cortos):

1. Reducir `max_new_tokens` a 128
2. Usar `BEAM_SIZE=2` (opcional)
3. Activar cachÃ© para requests repetidos

## ğŸ” Debugging

### Logs Ãºtiles

```python
# app/inference.py
logger.debug(f"max_new_tokens adaptativo: {old} â†’ {new}")
logger.info(f"Traduciendo {len(segments)} segmentos...")
```

### MÃ©tricas del frontend

```typescript
// Console del navegador
console.log("Latencia:", lastLatencyMs, "ms")
console.log("Cache:", info.cache.hit_rate)
```

### Health Check

```bash
curl http://localhost:8000/health
```

```json
{
  "status": "healthy",
  "model_loaded": true,
  "load_time_seconds": 15.23
}
```

## ğŸ“ Notas TÃ©cnicas

### HeurÃ­stica de max_new_tokens

```
salida_estimada = longitud_entrada Ã— 1.2
max_new_tokens = clamp(salida_estimada, 128, 512)
```

**Rationale**:
- Factor 1.2: DanÃ©s/EspaÃ±ol suelen tener longitud similar (~Â±20%)
- Min 128: Garantiza traducciones completas para textos cortos
- Max 512: LÃ­mite Pydantic y performance

### SegmentaciÃ³n

**Algoritmo**:
1. Detectar pÃ¡rrafos (`\n\n`)
2. Segmentar por oraciones (`. `, `? `, `! `)
3. Agrupar hasta `max_segment_chars`
4. Preservar delimitadores

**Ventajas**:
- No corta en medio de palabra
- Respeta estructura del texto
- Mantiene contexto local

### Cache

El cachÃ© incluye la direcciÃ³n en la clave:

```python
cache_key = f"{direction}||{texto_normalizado}"
```

**Impacto**:
- âœ… Segmentos repetidos â†’ cache hit
- âœ… Retraducciones instantÃ¡neas
- âœ… EstadÃ­sticas precisas por direcciÃ³n

## ğŸ“ Aprendizajes

### Â¿Por quÃ© no 2048 tokens?

**Razones**:
1. **Performance CPU**: INT8 en CPU es lento con secuencias muy largas
2. **Memoria**: Batch processing con secuencias largas consume mucha RAM
3. **Calidad**: SegmentaciÃ³n produce traducciones mÃ¡s coherentes
4. **Pragmatismo**: 1024 cubre >95% de casos de uso

### Â¿Por quÃ© segmentaciÃ³n automÃ¡tica?

**Alternativas consideradas**:
1. âŒ Aumentar MAX_INPUT_TOKENS a 4096 â†’ Performance inaceptable
2. âŒ Reducir beam_size â†’ PÃ©rdida de calidad
3. âœ… SegmentaciÃ³n inteligente â†’ Balance Ã³ptimo

## ğŸš§ Trabajo Futuro

### Optimizaciones pendientes

- [ ] **SegmentaciÃ³n por contexto**: Usar ventanas deslizantes con overlap
- [ ] **Streaming**: Devolver segmentos traducidos progresivamente
- [ ] **CachÃ© distribuido**: Redis/Memcached para multi-instancia
- [ ] **GPU opcional**: Detectar GPU y usar FP16 si estÃ¡ disponible
- [ ] **CompresiÃ³n**: Almacenar embeddings en lugar de texto completo

### Mejoras UX pendientes

- [ ] **Barra de progreso**: Para textos largos con mÃºltiples segmentos
- [ ] **EstimaciÃ³n de tiempo**: Basada en longitud y cache hits
- [ ] **Preview en tiempo real**: Mostrar segmentos mientras se traducen
- [ ] **Modo avanzado**: Exponer mÃ¡s parÃ¡metros (beam_size, repetition_penalty)

## ğŸ“š Referencias

- [NLLB Model Card](https://huggingface.co/facebook/nllb-200-distilled-600M)
- [CTranslate2 Docs](https://opennmt.net/CTranslate2/)
- [FastAPI Best Practices](https://fastapi.tiangolo.com/tutorial/)
- [React Performance](https://react.dev/learn/render-and-commit)

---

**Fecha**: Octubre 2025  
**VersiÃ³n**: 7.0 - Anti-truncado y Performance  
**Autor**: Sistema de traducciÃ³n ES â†” DA


"""
Motor de inferencia usando CTranslate2 para traducci√≥n NLLB.

Usa ModelManager para acceso al modelo y garantiza traducci√≥n determin√≠stica ES‚ÜíDA.
Incluye cach√© LRU para evitar retraducciones.
"""
import re
import logging
from typing import List, Optional

from app.settings import settings
from app.startup import model_manager
from app.cache import translation_cache
from app.postprocess_da import postprocess_da
from app.postprocess_es import postprocess_es
from app.utils_text import (
    normalize_preserving_newlines,
    translate_preserving_structure
)


# Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Nota: load_model() ahora est√° en ModelManager (app/startup.py)


def _derive_max_new_tokens(input_lengths: List[int]) -> int:
    """
    Calcula max_new_tokens generosamente para evitar truncado.
    
    NUEVA L√ìGICA: Siempre generar valores muy altos para asegurar traducci√≥n completa.
    
    Args:
        input_lengths: Lista de longitudes de input_ids
        
    Returns:
        max_new_tokens muy generoso (sin l√≠mites artificiales)
    """
    if not input_lengths:
        # Valor por defecto muy alto si no hay input
        return 4096
    
    max_input_len = max(input_lengths)
    
    # L√ìGICA MUY GENEROSA: factor m√≠nimo 3.0x y hasta 5.0x para textos largos
    if max_input_len <= 100:
        factor = 3.0  # Muy generoso incluso para textos cortos
    elif max_input_len <= 300:
        factor = 4.0  # Extremadamente generoso
    else:
        factor = 5.0  # M√°ximo generoso para textos largos
    
    estimated = int(max_input_len * factor)
    
    # M√≠nimo absoluto de 1024 tokens, sin m√°ximo
    return max(1024, estimated)


def _needs_continuation(tokens: List[str], max_tokens: int) -> bool:
    """
    Determina si una traducci√≥n necesita continuaci√≥n autom√°tica.
    
    L√ìGICA EXTREMADAMENTE AGRESIVA:
    - Detectar cualquier indicio de truncado posible
    
    Args:
        tokens: Lista de tokens generados
        max_tokens: L√≠mite m√°ximo usado
        
    Returns:
        True si necesita continuaci√≥n
    """
    if not tokens:
        return False
    
    logger.info(f"Evaluando continuaci√≥n: {len(tokens)}/{max_tokens} tokens")
    
    # CRITERIO 1: Longitud m√≠nima absoluta para textos largos
    # Si tenemos muchos tokens pero no terminamos bien, continuar
    if len(tokens) > 200:  # Para textos largos, ser m√°s estricto
        last_token = tokens[-1] if tokens else ""
        ending_punctuation = ['.', '!', '?', '‚Ä¶', '„ÄÇ', 'ÔºÅ', 'Ôºü', ')', '"', "'"]
        ends_properly = any(last_token.endswith(punct) for punct in ending_punctuation)
        
        if not ends_properly:
            logger.info(f"CRITERIO 1 - Necesita continuaci√≥n: texto largo sin fin apropiado: '{last_token}'")
            return True
    
    # CRITERIO 2: Threshold bajo para cualquier caso
    threshold = int(max_tokens * 0.3)  # Muy bajo: 30%
    if len(tokens) >= threshold:
        last_token = tokens[-1] if tokens else ""
        ending_punctuation = ['.', '!', '?', '‚Ä¶', '„ÄÇ', 'ÔºÅ', 'Ôºü', ')', '"', "'"]
        ends_properly = any(last_token.endswith(punct) for punct in ending_punctuation)
        
        if not ends_properly:
            logger.info(f"CRITERIO 2 - Necesita continuaci√≥n: threshold bajo sin fin apropiado: {len(tokens)}/{max_tokens}")
            return True
    
    # CRITERIO 3: Para textos de longitud media, verificar finalizaci√≥n natural
    if len(tokens) > 100:
        # Concatenar √∫ltimos tokens para verificar patr√≥n
        last_tokens_text = " ".join(tokens[-5:]).lower()
        
        # Detectar si parece incompleto
        incomplete_patterns = ["fordi", "s√•", "desuden", "derfor", "men", "og"]
        if any(pattern in last_tokens_text for pattern in incomplete_patterns):
            logger.info(f"CRITERIO 3 - Necesita continuaci√≥n: patr√≥n incompleto detectado")
            return True
    
    logger.info(f"No necesita continuaci√≥n")
    return False


def translate_batch(
    texts: List[str],
    direction: str = "es-da",
    max_new_tokens: Optional[int] = None,
    beam_size: Optional[int] = None,
    use_cache: bool = True,
    formal: bool = False,
    strict_max: bool = False,
    preserve_newlines: bool = True
) -> List[str]:
    """
    Traduce un batch de textos entre espa√±ol y dan√©s (bidireccional).
    
    Incluye cach√© LRU, post-procesado, validaci√≥n de salida y continuaci√≥n autom√°tica.
    Soporta ES‚ÜíDA y DA‚ÜíES.
    
    Args:
        texts: Lista de textos a traducir
        direction: Direcci√≥n de traducci√≥n ("es-da" o "da-es")
        max_new_tokens: M√°ximo n√∫mero de tokens a generar (None = auto-calculado)
        beam_size: Tama√±o del beam search (default: settings.BEAM_SIZE)
        use_cache: Si True, usa cach√© para evitar retraducciones
        formal: Si True, aplica estilo formal (solo para salida danesa)
        strict_max: Si True, NO elevar max_new_tokens ni hacer continuaci√≥n autom√°tica
        preserve_newlines: Si True, preserva todos los saltos de l√≠nea del original
        
    Returns:
        Lista de traducciones post-procesadas
        
    Raises:
        RuntimeError: Si el modelo no est√° cargado
        ValueError: Si direction es inv√°lida
        Exception: Si hay error en la traducci√≥n
    """
    # Validar direcci√≥n
    if direction not in ["es-da", "da-es"]:
        raise ValueError(f"Direcci√≥n inv√°lida: {direction}. Usa 'es-da' o 'da-es'")
    
    # Acceder al modelo via ModelManager
    if not model_manager.model_loaded:
        raise RuntimeError(
            "Modelo no cargado. El servidor arranc√≥ pero el modelo no est√° disponible. "
            "Revisa /health para diagn√≥stico."
        )
    
    translator = model_manager.translator
    tokenizer = model_manager.tokenizer
    
    if not texts:
        return []
    
    # Usar defaults de settings si no se especifican
    if beam_size is None:
        beam_size = settings.BEAM_SIZE
    
    # max_new_tokens se calcular√° despu√©s de tokenizar (necesitamos input_lengths)
    
    # Configurar idiomas seg√∫n direcci√≥n
    if direction == "es-da":
        src_lang = "spa_Latn"
        tgt_lang = "dan_Latn"
    else:  # da-es
        src_lang = "dan_Latn"
        tgt_lang = "spa_Latn"
    
    # Obtener token BOS del idioma target
    if not hasattr(tokenizer, 'lang_code_to_id'):
        raise RuntimeError("Tokenizador no soporta lang_code_to_id (no es NLLB)")
    
    tgt_lang_id = tokenizer.lang_code_to_id.get(tgt_lang)
    if tgt_lang_id is None:
        raise RuntimeError(f"No se encontr√≥ token para idioma {tgt_lang}")
    
    tgt_bos_tok = tokenizer.convert_ids_to_tokens(tgt_lang_id)
    
    # Separar textos en cach√© vs no cach√© (con direcci√≥n)
    translations = [None] * len(texts)
    texts_to_translate = []
    indices_to_translate = []
    
    if use_cache:
        for i, text in enumerate(texts):
            # Incluir direcci√≥n en la clave del cach√©
            cache_key = f"{direction}||{text}"
            cached = translation_cache.get(cache_key)
            if cached is not None:
                translations[i] = cached
            else:
                texts_to_translate.append(text)
                indices_to_translate.append(i)
    else:
        texts_to_translate = texts
        indices_to_translate = list(range(len(texts)))
    
    # Si no hay nada que traducir (todo en cach√©), retornar
    if not texts_to_translate:
        logger.info(f"Cache: 100% hits ({len(texts)} textos)")
        return translations
    
    if use_cache:
        logger.info(f"Cache: {len(translations) - len(texts_to_translate)} hits, {len(texts_to_translate)} misses")
    
    try:
        # Pre-procesar textos: normalizar espacios
        texts_normalized = [
            _normalize_text(text, preserve_newlines=preserve_newlines) 
            for text in texts_to_translate
        ]
        
        # Configurar idioma source en el tokenizador
        if hasattr(tokenizer, 'src_lang'):
            tokenizer.src_lang = src_lang
            logger.debug(f"Idioma source configurado: {src_lang}")
        
        # Tokenizar textos de entrada SIN TORCH (solo listas de IDs)
        # NLLB espera source language token al inicio
        # El tokenizador ya a√±ade este token autom√°ticamente si src_lang est√° configurado
        # Usar l√≠mite muy alto para evitar truncado de entrada
        safe_input_limit = max(8192, settings.MAX_INPUT_TOKENS)
        logger.info(f"üîß Tokenizando con l√≠mite de entrada: {safe_input_limit}")
        
        encoded = tokenizer(
            texts_normalized,
            padding=True,
            truncation=True,  # Mantener pero con l√≠mite muy alto
            max_length=safe_input_limit,
            return_attention_mask=False,
            return_token_type_ids=False
        )
        
        # input_ids ya es una lista de listas (sin tensores)
        input_ids_list = encoded["input_ids"]
        
        # Calcular/elevar max_new_tokens seg√∫n l√≥gica adaptativa + elevaci√≥n server-side
        input_lengths = [len(ids) for ids in input_ids_list]
        derived = _derive_max_new_tokens(input_lengths)
        
        # Debug logging mejorado para investigar truncado
        logger.info(f"üîç INFERENCE DEBUG - max_new_tokens recibido: {max_new_tokens}")
        logger.info(f"üîç INFERENCE DEBUG - input_lengths: {input_lengths}")
        logger.info(f"üîç INFERENCE DEBUG - derived: {derived}")
        logger.info(f"üîç INFERENCE DEBUG - strict_max: {strict_max}")
        
        if max_new_tokens is None:
            # Cliente no especific√≥: usar valor extremadamente alto para evitar truncado
            max_new_tokens = max(4096, derived)  # M√≠nimo 4096 tokens
            logger.info(f"üîÑ max_new_tokens auto-calculado (EXTREMO): {max_new_tokens}")
        else:
            # Cliente especific√≥ un valor
            if strict_max:
                # Respetar exactamente el valor del cliente
                logger.info(f"üîí max_new_tokens (strict): {max_new_tokens}")
            else:
                # SIEMPRE elevar a valores extremadamente altos para evitar truncado
                original = max_new_tokens
                max_new_tokens = max(max_new_tokens, max(4096, derived))  # M√≠nimo 4096 siempre
                if max_new_tokens != original:
                    logger.info(f"üìà max_new_tokens elevado autom√°ticamente: {original} ‚Üí {max_new_tokens} (extremo)")
        
        # NO aplicar l√≠mites artificiales - permitir traducci√≥n completa
        # max_new_tokens ya est√° calculado correctamente arriba
        
        logger.info(f"üöÄ FINAL - Usando max_new_tokens: {max_new_tokens}")
        
        # Convertir IDs a tokens para CTranslate2
        source_tokens = [
            tokenizer.convert_ids_to_tokens(ids)
            for ids in input_ids_list
        ]
        
        # Preparar target_prefix con token de idioma destino
        # NLLB requiere el token del idioma destino al inicio de la generaci√≥n
        if not tgt_bos_tok:
            raise RuntimeError(
                f"Token de idioma {tgt_lang} no configurado. "
                "Verifica que el modelo NLLB est√© correctamente cargado."
            )
        
        target_prefix = [[tgt_bos_tok]] * len(texts_to_translate)
        
        # Traducir con CTranslate2 usando valores muy altos para evitar truncado
        # Asegurar que nunca se limita artificialmente
        safe_max_tokens = max(8192, max_new_tokens)  # M√≠nimo 8192 tokens
        logger.info(f"üîß Usando max_decoding_length: {safe_max_tokens}")
        
        results = translator.translate_batch(
            source_tokens,
            target_prefix=target_prefix,
            beam_size=beam_size,
            max_decoding_length=safe_max_tokens,
            return_scores=False,
            repetition_penalty=1.2,  # Evitar repeticiones
            no_repeat_ngram_size=3   # Evitar repetici√≥n de 3-gramas
        )
        
        # Extraer hip√≥tesis (primera de cada beam)
        hypotheses = [result.hypotheses[0] for result in results]
        
        # Continuaci√≥n autom√°tica SIEMPRE para textos largos (sin l√≠mites)
        if not strict_max:
            continuation_indices = []
            for i, tokens in enumerate(hypotheses):
                # L√ìGICA SIMPLE: Si el texto original era largo, hacer continuaci√≥n SIEMPRE
                original_text = texts_to_translate[i] if i < len(texts_to_translate) else ""
                is_long_text = len(original_text) > 500  # Texto de m√°s de 500 chars
                
                if is_long_text or _needs_continuation(tokens, safe_max_tokens):
                    continuation_indices.append(i)
                    logger.info(f"Item {i}: candidato para continuaci√≥n (long_text={is_long_text}, needs_cont={_needs_continuation(tokens, safe_max_tokens)})")
            
            if continuation_indices:
                logger.info(f"üîÑ Continuaci√≥n autom√°tica para {len(continuation_indices)} item(s)")
                
                for idx in continuation_indices:
                    # Target prefix = tokens ya generados
                    prefix_tokens = hypotheses[idx]
                    logger.info(f"Continuando item {idx}: tokens actuales={len(prefix_tokens)}")
                    
                    # EXTREMADAMENTE generoso para continuaci√≥n - sin l√≠mites artificiales
                    new_max = 16384  # Valor fijo muy alto
                    
                    # Segunda pasada con los tokens previos como prefix
                    continuation_result = translator.translate_batch(
                        [source_tokens[idx]],
                        target_prefix=[[tgt_bos_tok] + prefix_tokens],  # incluir BOS + tokens previos
                        beam_size=beam_size,
                        max_decoding_length=new_max,  # Sin restar - usar valor alto completo
                        return_scores=False,
                        repetition_penalty=1.2,
                        no_repeat_ngram_size=3
                    )
                    
                    # Obtener tokens de continuaci√≥n
                    continuation_tokens = continuation_result[0].hypotheses[0]
                    
                    # Concatenar (evitar duplicar el primer token si el decoder lo repite)
                    if continuation_tokens and continuation_tokens[0] == prefix_tokens[-1]:
                        continuation_tokens = continuation_tokens[1:]
                    
                    hypotheses[idx] = prefix_tokens + continuation_tokens
                    logger.info(f"Item {idx}: continuaci√≥n agreg√≥ {len(continuation_tokens)} tokens. Total: {len(hypotheses[idx])}")
        
        # Convertir tokens a texto y post-procesar
        new_translations = []
        for i, tokens in enumerate(hypotheses):
            # Convertir tokens a IDs
            token_ids = tokenizer.convert_tokens_to_ids(tokens)
            
            # Decodificar a texto
            text = tokenizer.decode(
                token_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            
            # Post-procesamiento: limpiar artefactos
            text = _clean_translation(text)
            
            # Validaci√≥n: verificar que la salida es alfabeto latino
            if not is_mostly_latin(text):
                logger.warning(
                    f"Salida con caracteres no latinos detectada (segmento {i+1}). "
                    f"Reintentando con beam_size={min(beam_size + 1, 5)}..."
                )
                # Reintentar UNA VEZ con beam_size mayor
                if beam_size < 5:
                    retry_result = translate_batch(
                        [texts_to_translate[i]], 
                        max_new_tokens=max_new_tokens, 
                        beam_size=min(beam_size + 1, 5),
                        use_cache=False,  # No usar cach√© en reintentos
                        formal=formal
                    )
                    text = retry_result[0]
                else:
                    # Error controlado si persiste
                    raise ValueError(
                        f"No se pudo obtener salida en alfabeto latino. "
                        f"Texto original: {texts_to_translate[i][:100]}..."
                    )
            
            # Post-procesado seg√∫n idioma destino
            if direction == "es-da":
                text = postprocess_da(text, formal=formal)
            else:  # da-es
                text = postprocess_es(text)
            
            new_translations.append(text)
            
            # Guardar en cach√© (con direcci√≥n)
            if use_cache:
                cache_key = f"{direction}||{texts_to_translate[i]}"
                translation_cache.put(cache_key, text)
        
        # Insertar traducciones nuevas en las posiciones correctas
        for i, idx in enumerate(indices_to_translate):
            translations[idx] = new_translations[i]
        
        return translations
        
    except Exception as e:
        logger.error(f"Error en traducci√≥n: {e}", exc_info=True)
        raise Exception(f"Error al traducir: {str(e)}")


def _normalize_text(text: str, preserve_newlines: bool = True) -> str:
    """
    Normaliza el texto de entrada.
    
    Args:
        text: Texto a normalizar
        preserve_newlines: Si True, preserva TODOS los saltos de l√≠nea.
                          Si False, usa normalizaci√≥n legacy (limita a 2 saltos)
    
    Returns:
        Texto normalizado
    """
    if preserve_newlines:
        # Usar nueva l√≥gica que preserva TODA la estructura
        return normalize_preserving_newlines(text)
    else:
        # L√≥gica legacy: normalizar espacios y limitar saltos de l√≠nea
        # Normalizar espacios m√∫ltiples PERO preservar saltos de l√≠nea
        text = re.sub(r'[ \t]+', ' ', text)  # Solo espacios y tabs, no \n
        # Normalizar saltos de l√≠nea m√∫ltiples a m√°ximo 2 consecutivos
        text = re.sub(r'\n{3,}', '\n\n', text)
        # Eliminar espacios al inicio y final de l√≠neas
        lines = text.split('\n')
        lines = [line.strip() for line in lines]
        text = '\n'.join(lines)
        return text


def _clean_translation(text: str) -> str:
    """
    Limpia artefactos en la traducci√≥n generada.
    
    - Elimina tokens de idioma visibles si aparecen (ej: "dan_Latn")
    - Normaliza espacios
    """
    # Eliminar posibles tokens de idioma que se hayan colado
    text = re.sub(r'\b(dan_Latn|spa_Latn)\b', '', text)
    
    # Eliminar marcadores residuales BOS/EOS si aparecen como texto
    text = re.sub(r'<\|.*?\|>', '', text)
    
    # Normalizar espacios m√∫ltiples resultantes
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    return text


def is_mostly_latin(text: str) -> bool:
    """
    Valida que el texto contenga principalmente caracteres del alfabeto latino.
    
    Permite letras latinas (incluyendo danesas: √¶, √∏, √•), n√∫meros, 
    puntuaci√≥n com√∫n y algunos s√≠mbolos.
    
    Args:
        text: Texto a validar
    
    Returns:
        True si >80% son caracteres latinos v√°lidos, False en caso contrario
    """
    if not text:
        return True
    
    # Caracteres permitidos: latinos, daneses, n√∫meros, puntuaci√≥n, espacios
    # Incluye: a-z, A-Z, √¶√∏√•√Ü√ò√Ö, n√∫meros, puntuaci√≥n com√∫n, espacios
    latin_pattern = re.compile(
        r'[a-zA-Z√¶√∏√•√Ü√ò√Ö√†√°√¢√£√§√•√®√©√™√´√¨√≠√Æ√Ø√≤√≥√¥√µ√∂√π√∫√ª√º√Ω√ø√±√ß√Ä√Å√Ç√É√Ñ√Ö√à√â√ä√ã√å√ç√é√è√í√ì√î√ï√ñ√ô√ö√õ√ú√ù≈∏√ë√á0-9\s\.,;:!?¬ø¬°\-\'\"()\[\]{}/@#$%&*+=<>|\\~`]'
    )
    
    # Contar caracteres latinos vs total
    latin_chars = len(latin_pattern.findall(text))
    total_chars = len(text)
    
    if total_chars == 0:
        return True
    
    ratio = latin_chars / total_chars
    
    # Si m√°s del 20% son caracteres no latinos, considerar inv√°lido
    if ratio < 0.8:
        logger.warning(
            f"Texto con baja proporci√≥n de caracteres latinos: {ratio:.2%}. "
            f"Muestra: {text[:100]}"
        )
        return False
    
    return True


def translate_text_preserving_structure(
    text: str,
    direction: str = "es-da",
    max_new_tokens: Optional[int] = None,
    formal: bool = False,
    strict_max: bool = False
) -> str:
    """
    Traduce un texto preservando TODA su estructura de saltos de l√≠nea.
    
    Divide el texto por bloques de p√°rrafos (separados por \\n\\n+) y traduce
    cada bloque independientemente, luego reensambla usando los separadores originales.
    
    Args:
        text: Texto a traducir
        direction: Direcci√≥n de traducci√≥n ("es-da" o "da-es")
        max_new_tokens: M√°ximo de tokens por bloque (None = auto)
        formal: Aplicar estilo formal
        strict_max: No elevar max_new_tokens autom√°ticamente
        
    Returns:
        Texto traducido con estructura preservada
    """
    def translate_block(block: str) -> str:
        """Funci√≥n interna para traducir un bloque individual."""
        result = translate_batch(
            [block],
            direction=direction,
            max_new_tokens=max_new_tokens,
            use_cache=True,
            formal=formal,
            strict_max=strict_max,
            preserve_newlines=True
        )
        return result[0] if result else block
    
    # Usar utilidad de preservaci√≥n de estructura
    return translate_preserving_structure(text, translate_block)


# Nota: get_model_info() ahora est√° en ModelManager.health() (app/startup.py)


# ========================================
# Configuración del Traductor ES→DA
# ========================================
# Copia este archivo como .env y ajusta según necesites

# ========================================
# CONFIGURACIÓN DEL MODELO
# ========================================

# Modelo a usar desde HuggingFace
# Opciones:
#   - facebook/nllb-200-distilled-600M (recomendado para 8GB+ RAM)
#   - facebook/nllb-200-1.3B (recomendado para 16GB+ RAM)
#   - facebook/nllb-200-3.3B (recomendado para 32GB+ RAM)
MODEL_NAME=facebook/nllb-200-distilled-600M

# Directorio donde se descarga el modelo HuggingFace original
MODEL_DIR=./models/nllb-600m

# Directorio del modelo convertido a CTranslate2 INT8
CT2_DIR=./models/nllb-600m-ct2-int8

# ========================================
# CONFIGURACIÓN DE PERFORMANCE
# ========================================

# Hilos para paralelismo entre traducciones (batch processing)
# 0 = automático (detecta cores disponibles)
# Recomendado: 0 o número de cores / 2
CT2_INTER_THREADS=0

# Hilos para paralelismo dentro de cada traducción
# 0 = automático (detecta cores disponibles)
# Recomendado: 0 o número de cores / 2
CT2_INTRA_THREADS=0

# Tamaño de batch por defecto para procesamiento
# Mayor = más throughput pero más RAM
# Recomendado: 8-16 para modelos 600M, 4-8 para modelos 1.3B
DEFAULT_BATCH_SIZE=16

# ========================================
# CONFIGURACIÓN DEL SERVIDOR
# ========================================

# Puerto del servidor FastAPI
PORT=8000

# Host del servidor (0.0.0.0 para acceso externo, 127.0.0.1 solo local)
HOST=0.0.0.0

# Nivel de logging (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ========================================
# NOTAS
# ========================================
#
# Para usar el modelo 1.3B (mejor calidad):
#   MODEL_NAME=facebook/nllb-200-1.3B
#   MODEL_DIR=./models/nllb-1.3b
#   CT2_DIR=./models/nllb-1.3b-ct2-int8
#   DEFAULT_BATCH_SIZE=8
#
# Luego ejecuta:
#   make download
#   make convert
#
# ========================================

